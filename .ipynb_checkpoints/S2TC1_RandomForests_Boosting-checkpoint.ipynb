{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Construcción e implementación de modelos Bagging, Random Forest y XGBoost\n",
    "\n",
    "En este taller podrán poner en práctica sus conocimientos sobre la construcción e implementación de modelos de Bagging, Random Forest y XGBoost. El taller está constituido por 8 puntos, en los cuales deberan seguir las intrucciones de cada numeral para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos predicción precio de automóviles\n",
    "\n",
    "En este taller se usará el conjunto de datos de Car Listings de Kaggle donde cada observación representa el precio de un automóvil teniendo en cuenta distintas variables como año, marca, modelo, entre otras. El objetivo es predecir el precio del automóvil. Para más detalles puede visitar el siguiente enlace: [datos](https://www.kaggle.com/jpayne/852k-used-car-listings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Year</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>M_Camry</th>\n",
       "      <th>M_Camry4dr</th>\n",
       "      <th>M_CamryBase</th>\n",
       "      <th>M_CamryL</th>\n",
       "      <th>M_CamryLE</th>\n",
       "      <th>M_CamrySE</th>\n",
       "      <th>M_CamryXLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21995</td>\n",
       "      <td>2014</td>\n",
       "      <td>6480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13995</td>\n",
       "      <td>2014</td>\n",
       "      <td>39972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>17941</td>\n",
       "      <td>2016</td>\n",
       "      <td>18989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>12493</td>\n",
       "      <td>2014</td>\n",
       "      <td>51330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>7994</td>\n",
       "      <td>2007</td>\n",
       "      <td>116065</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Year  Mileage  M_Camry  M_Camry4dr  M_CamryBase  M_CamryL  \\\n",
       "7    21995  2014     6480        0           0            0         1   \n",
       "11   13995  2014    39972        0           0            0         0   \n",
       "167  17941  2016    18989        0           0            0         0   \n",
       "225  12493  2014    51330        0           0            0         1   \n",
       "270   7994  2007   116065        0           1            0         0   \n",
       "\n",
       "     M_CamryLE  M_CamrySE  M_CamryXLE  \n",
       "7            0          0           0  \n",
       "11           1          0           0  \n",
       "167          0          1           0  \n",
       "225          0          0           0  \n",
       "270          0          0           0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importación de librerías\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "# Lectura de la información de archivo .csv\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/dataTrain_carListings.zip')\n",
    "\n",
    "# Preprocesamiento de datos para el taller\n",
    "data = data.loc[data['Model'].str.contains('Camry')].drop(['Make', 'State'], axis=1)\n",
    "data = data.join(pd.get_dummies(data['Model'], prefix='M'))\n",
    "data = data.drop(['Model'], axis=1)\n",
    "\n",
    "# Visualización dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y)\n",
    "y = data['Price']\n",
    "X = data.drop(['Price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en set de entrenamiento y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year           7031\n",
       "Mileage        7031\n",
       "M_Camry        7031\n",
       "M_Camry4dr     7031\n",
       "M_CamryBase    7031\n",
       "M_CamryL       7031\n",
       "M_CamryLE      7031\n",
       "M_CamrySE      7031\n",
       "M_CamryXLE     7031\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Árbol de decisión manual\n",
    "\n",
    "En la celda 1 creen un árbol de decisión **manualmente**  que considere los set de entrenamiento y test definidos anteriormente y presenten el RMSE y MAE del modelo en el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para realizar el calculo manual del arbol, se hace uso de los ejemplo expuestos en el laboratorioa 1 de la semana 1.\n",
    "\n",
    "# Definición de la función que calcula el gini index\n",
    "def gini(y):\n",
    "    if y.shape[0] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - (y.mean()**2 + (1 - y.mean())**2)\n",
    "    \n",
    "# Definición de la función gini_imputiry para calular la ganancia de una variable predictora j dado el punto de corte k\n",
    "def gini_impurity(X_col, y, split):\n",
    "    \n",
    "    filter_l = X_col < split\n",
    "    y_l = y.loc[filter_l]\n",
    "    y_r = y.loc[~filter_l]\n",
    "    \n",
    "    n_l = y_l.shape[0]\n",
    "    n_r = y_r.shape[0]\n",
    "    \n",
    "    gini_y = gini(y)\n",
    "    gini_l = gini(y_l)\n",
    "    gini_r = gini(y_r)\n",
    "    \n",
    "    gini_impurity_ = gini_y - (n_l / (n_l + n_r) * gini_l + n_r / (n_l + n_r) * gini_r)\n",
    "    \n",
    "    return gini_impurity_\n",
    "\n",
    "\n",
    "# Definición de la función best_split para calcular cuál es la mejor variable y punto de cortepara hacer la bifurcación del árbol\n",
    "def best_split(X, y, num_pct=10):\n",
    "    \n",
    "    features = range(X.shape[1])\n",
    "    \n",
    "    best_split = [0, 0, 0]  # j, split, gain\n",
    "    \n",
    "    # Para todas las varibles \n",
    "    for j in features:\n",
    "        \n",
    "        splits = np.percentile(X.iloc[:, j], np.arange(0, 100, 100.0 / (num_pct+1)).tolist())\n",
    "        splits = np.unique(splits)[1:]\n",
    "        \n",
    "        # Para cada partición\n",
    "        for split in splits:\n",
    "            gain = gini_impurity(X.iloc[:, j], y, split)\n",
    "                        \n",
    "            if gain > best_split[2]:\n",
    "                best_split = [j, split, gain]\n",
    "    \n",
    "    return best_split\n",
    "\n",
    "\n",
    "# Definición de la función tree_grow para hacer un crecimiento recursivo del árbol\n",
    "def tree_grow(X, y, level=0, min_gain=0.001, max_depth=None, num_pct=10):\n",
    "    \n",
    "    # Si solo es una observación\n",
    "    if X.shape[0] == 1:\n",
    "        tree = dict(y_pred=y.iloc[:1].values[0], y_prob=0.5, level=level, split=-1, n_samples=1, gain=0)\n",
    "        return tree\n",
    "    \n",
    "    # Calcular la mejor división\n",
    "    j, split, gain = best_split(X, y, num_pct)\n",
    "    \n",
    "    # Guardar el árbol y estimar la predicción\n",
    "    y_pred = int(y.mean() >= 0.5) \n",
    "    y_prob = (y.sum() + 1.0) / (y.shape[0] + 2.0)  # Corrección Laplace \n",
    "    \n",
    "    tree = dict(y_pred=y_pred, y_prob=y_prob, level=level, split=-1, n_samples=X.shape[0], gain=gain)\n",
    "    # Revisar el criterio de parada \n",
    "    if gain < min_gain:\n",
    "        return tree\n",
    "    if max_depth is not None:\n",
    "        if level >= max_depth:\n",
    "            return tree   \n",
    "    \n",
    "    # Continuar creando la partición\n",
    "    filter_l = X.iloc[:, j] < split\n",
    "    X_l, y_l = X.loc[filter_l], y.loc[filter_l]\n",
    "    X_r, y_r = X.loc[~filter_l], y.loc[~filter_l]\n",
    "    tree['split'] = [j, split]\n",
    "\n",
    "    # Siguiente iteración para cada partición\n",
    "    \n",
    "    tree['sl'] = tree_grow(X_l, y_l, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    tree['sr'] = tree_grow(X_r, y_r, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# Aplicación de la función tree_grow\n",
    "tree = tree_grow(X_train, y_train, level=0, min_gain=0.001, max_depth=1, num_pct=10)\n",
    "\n",
    "# Definición de la función tree_predict para hacer predicciones según las variables 'X' y el árbol 'tree'\n",
    "\n",
    "def tree_predict(X, tree, proba=False):\n",
    "    \n",
    "    predicted = np.ones(X.shape[0])\n",
    "\n",
    "    # Revisar si es el nodo final\n",
    "    if tree['split'] == -1:\n",
    "        if not proba:\n",
    "            predicted = predicted * tree['y_pred']\n",
    "        else:\n",
    "            predicted = predicted * tree['y_prob']\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        j, split = tree['split']\n",
    "        filter_l = (X.iloc[:, j] < split)\n",
    "        X_l = X.loc[filter_l]\n",
    "        X_r = X.loc[~filter_l]\n",
    "\n",
    "        if X_l.shape[0] == 0:  # Si el nodo izquierdo está vacio solo continua con el derecho \n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'], proba)\n",
    "        elif X_r.shape[0] == 0:  #  Si el nodo derecho está vacio solo continua con el izquierdo\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'], proba)\n",
    "        else:\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'], proba)\n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'], proba)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "# Ejecución de función tree_predict\n",
    "y_pred = tree_predict(X_test, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15113.767253661932\n",
      "14598.966224018475\n"
     ]
    }
   ],
   "source": [
    "#y_predicho\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(rmse)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2 - Bagging manual\n",
    "\n",
    "En la celda 2 creen un modelo bagging **manualmente** con 10 árboles de regresión y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arreglo: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Muestreo aleatorio:  [ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2149.343650928659"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 2\n",
    "\n",
    "# Se crea un arreglo de 1 a 20\n",
    "np.random.seed(1)\n",
    "\n",
    "# Impresión de arreglo y muestreo aleatorio\n",
    "nums = np.arange(1, 21)\n",
    "print('Arreglo:', nums)\n",
    "print('Muestreo aleatorio: ', np.random.choice(a=nums, size=20, replace=True))\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_B = 10\n",
    "\n",
    "samples = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(1, n_B +1 )]\n",
    "samples\n",
    "\n",
    "# Visualización muestra boostrap #1 para entremiento\n",
    "X_train.iloc[samples[0], :]\n",
    "\n",
    "# Construcción un árbol de decisión para cada muestra boostrap\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Definición del modelo usando DecisionTreeRegressor de sklearn\n",
    "treereg = DecisionTreeRegressor(max_depth=None, random_state=123)\n",
    "\n",
    "# DataFrame para guardar las predicciones de cada árbol\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=[list(range(n_B))])\n",
    "\n",
    "# Entrenamiento de un árbol sobre cada muestra boostrap y predicción sobre los datos de test\n",
    "for i, sample in enumerate(samples):\n",
    "    treereg.fit(X_train, y_train)\n",
    "    y_pred.iloc[:,i] = treereg.predict(X_test)\n",
    "    \n",
    "y_pred\n",
    "\n",
    "    \n",
    "# Error al promediar las predicciones de todos los árboles\n",
    "np.sqrt(mean_squared_error(y_test, y_pred.mean(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Bagging \n",
    "\n",
    "Se realiza la implementación manual del arbol de decisión y bagging de forma manual según lo solicitado, a nivel del redimiento del modelo en promedio tiene un MSE de 2149.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3 - Bagging con librería\n",
    "\n",
    "En la celda 3, con la librería sklearn, entrenen un modelo bagging con 10 árboles de regresión y el parámetro `max_features` igual a `log(n_features)` y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging con librería\n",
      "R^2: 0.7851015854948061\n"
     ]
    }
   ],
   "source": [
    "# Celda 3\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "max_features = int(np.log(n_features))\n",
    "base_estimator = DecisionTreeRegressor(max_features=max_features)\n",
    "model = BaggingRegressor(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2_bagging_libreria = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Bagging con librería\")\n",
    "print(\"R^2:\", r2_bagging_libreria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Bagging con libreria \n",
    "Después de definir y ejecutar el modelo con los parámetros establecidos, se puede observar que éste tiene la capacidad de interpretar el 78,5% de los datos del conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Random forest con librería\n",
    "\n",
    "En la celda 4, usando la librería sklearn entrenen un modelo de Randon Forest para regresión  y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest con librería\n",
      "R^2: 0.7962441677396794\n",
      "Score en el conjunto de prueba: 0.7962441677396794\n"
     ]
    }
   ],
   "source": [
    "# Celda 4\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2_random_forest_libreria = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Random forest con librería\")\n",
    "print(\"R^2:\", r2_random_forest_libreria)\n",
    "\n",
    "print(\"Score en el conjunto de prueba:\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Random forest con librería\n",
    "\n",
    "Después de definir y ejecutar el modelo con parámetros por defecto sin realizar ningún tipo de calidbración, se puede observar que éste tiene la capacidad de interpretar el 79,6% de los datos del conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Calibración de parámetros Random forest\n",
    "\n",
    "En la celda 5, calibren los parámetros max_depth, max_features y n_estimators del modelo de Randon Forest para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibración de parámetros Random forest\n",
      "R^2: 0.840083062290209\n",
      "Mejores parámetros de la calibración: {'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# Celda 5\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "param_grid = {'max_depth': [5, 10, 15, None],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'n_estimators': [50, 100, 200, 300, 400, 500]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir el resultado de la calibración \n",
    "\n",
    "r2_random_forest_libreria_calibracion = grid_search.score(X_test, y_test)\n",
    "print(\"Calibración de parámetros Random forest\")\n",
    "print(\"R^2:\", r2_random_forest_libreria_calibracion)\n",
    "print(\"Mejores parámetros de la calibración:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Random forest con librería y calibración de parámetros \n",
    "\n",
    "Se realiza la calibraicón de los parámetros principalmente la profundidad de los arboles (max_depth), cantidad de caracteristicas (max_features) y los estimadores (n_estimators), con esta calibración se identifica que los mejores parámetros son  'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 500, dando como resultado que el modelo tiene la capacidad de interpretar el 84% de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6 - XGBoost con librería\n",
    "\n",
    "En la celda 6 implementen un modelo XGBoost de regresión con la librería sklearn y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost con librería\n",
      "R^2: 0.8281265175296673\n"
     ]
    }
   ],
   "source": [
    "# Celda 6\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "r2_xgboost_libreria = model.score(X_test, y_test)\n",
    "\n",
    "print(\"XGBoost con librería\")\n",
    "print(\"R^2:\", r2_xgboost_libreria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis  XGBoost con librería\n",
    "Después de definir y ejecutar el modelo con los parámetros establecidos, se puede observar que éste tiene la capacidad de interpretar el 82,8% de los datos del conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 7 - Calibración de parámetros XGBoost\n",
    "\n",
    "En la celda 7 calibren los parámetros learning rate, gamma y colsample_bytree del modelo XGBoost para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibración de parámetros XGBoost\n",
      "R^2: 0.8432819349654288\n",
      "Mejores parámetros de la calibración: {'colsample_bytree': 0.5, 'gamma': 0, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Celda 7\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_grid = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "              'gamma': [0, 0.1, 0.01, 0.001],\n",
    "              'colsample_bytree': [0.5, 0.7, 0.9, 1]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "r2_xgboost_libreria_calibracion = grid_search.score(X_test, y_test)\n",
    "print(\"Calibración de parámetros XGBoost\")\n",
    "print(\"R^2:\", r2_xgboost_libreria_calibracion)\n",
    "print(\"Mejores parámetros de la calibración:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 8 - Comparación y análisis de resultados\n",
    "En la celda 8 comparen los resultados obtenidos de los diferentes modelos (random forest y XGBoost) y comenten las ventajas del mejor modelo y las desventajas del modelo con el menor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Celda 8\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r2_values = [r2_random_forest_libreria, r2_random_forest_libreria_calibracion, r2_xgboost_libreria, r2_xgboost_libreria_calibracion]\n",
    "model_names = ['Random Forest', 'Ca. Random Forest', 'XGBoost', 'Ca. XGBoost']\n",
    "\n",
    "plt.plot(model_names, r2_values, marker='o')\n",
    "plt.ylim([0.7, 1])\n",
    "plt.ylabel('R^2')\n",
    "plt.title('R^2 de diferentes modelos')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis del resultado de los diferentes modelos \n",
    "\n",
    "Tras realizar la implementacón de los diferentes modelos, se puede evidenciar que los R2 de cada uno son bastantes buenos, con diferencias mímimas entre si, siendo el valor más bajo de R2 0.7962 para Random Forest, mientras que el valor más alto es 0.8433 para XGBoost (Calibración).\n",
    "\n",
    "A nivel de rendimiento en la calibración del XGBoots fue minima respecto al Random Forest y el R2 del XGBoots fue mucho mejor respecto a Ramdom Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
